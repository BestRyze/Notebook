策略梯度中，我们要最大化期望奖励R。

这里r表示轨迹，R是一回合的总奖励，r\~p(θ)表示路径r由智能体θ生成，p<sub>θ</sub>(r) 是智能体θ走轨迹r的概率。
$$
\begin{align}

p_\theta(r) &= p(s_1)p_\theta(a_1|s_1)p(s2|(a_1,s_1))p_\theta(a_2|s_2)p(s_3|(a_2,s_2) \notag \\
            &= p(s_1)\prod_{t=1}\limits^T [p_\theta(a_t|s_t)p(s_{t+1}|(a_1,s_1))] \label{eq1}
            
\end{align}
$$

$$
\begin{align}\
        
\overline{R_\theta} &= E_{r \sim\ p(\theta)}[R(r)]\notag \\
              
             &= \sum_r R(r)p_{\theta}(r)  \label{eq2}


\end{align}
$$

R(r)只与环境有关，因此是常数，由公式\eqref{eq2}可以直接列出下式。
$$
\nabla \overline{R_\theta} = \sum_r R(r) \nabla [p_{\theta}(r)] \label{eq3}
$$


对于任意f(x), 满足公式\eqref{eq4}
$$
\nabla f(x) = f(x) \nabla \ln f(x)  \label{eq4}
$$

将式\eqref{eq4}代入\eqref{eq3}中，可得：
$$
\nabla \overline{R_\theta} = \sum_r R(r)p_\theta(r)\nabla\ln p_\theta(r) \label{eq5}
$$
因为很多时候不可能采样到所有路径，因此将式\eqref{eq5}进行以下近似,N为采样次数,注意，r的上标n指的是r的索引，不是指数
$$
\nabla \overline{R_\theta} \approx \frac{1}{N}\sum_{n=1}\limits^N \left[R(r^n)\nabla\ln p_\theta (r^n) \right] \label{eq6}
$$
由式\eqref{eq2}可得
$$
\ln p_\theta(r) = \ln p(s1) + \sum_{t=1}\limits^T\ln p(s_{t+1}|(s_t,a_t)) 
                    + \sum_{t=1}\limits^T\ln p_\theta(a_t|s_t) \label{eq7}
$$
式\eqref{eq7}的前两项只与环境有关，因此是常数，在求梯度的过程中可以被省略，因此:
$$
\nabla \ln p_\theta(r) = \nabla\sum_{t=1}\limits^T\ln p_\theta(a_t|s_t) \label{eq8}
$$
将\eqref{eq8}代入\eqref{eq6}可得：
$$
\begin{align}

\nabla \overline{R_\theta} &\approx \frac{1}{N}\sum_{n=1}\limits^N \left[R(r^n)\nabla\ln p_\theta (r^n) \right]\notag \\
                           &= \frac{1}{N}\sum_{n=1}\limits^N \left[R(r^n)\sum_{t=1}\limits^T\nabla\ln p_\theta(a_t|s_t) \right]
\end{align}
$$
手撕完毕，接下来该写代码了！！！